---
title: Majorizing Least Squares Loss functions
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
```

\sectionbreak 

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/majorls> 

\sectionbreak

# Introduction

The data of our problem are

* $x$, a vector with $n$ elements, 
* $W$, a positive semi-definite matrix of order $n$, 
* $\mathcal{Y}$, some subset of $\mathbb{R}^n$.

Define the least squares loss function
$$
\sigma(y):=\tfrac12(x-y)'W(x-y).
$$
Our problem is to minimize $\sigma$ over $y\in\mathcal{Y}$.

\sectionbreak

# Majorization

## General

Suppose $\tilde y$ is some element of $\mathcal{Y}$, with loss function value
$\sigma(\tilde y)$. We want to find an element of $\mathcal{Y}$ which is
better than $\tilde y$, i.e. has a lower value of loss.

::: {#lem-major}
Suppose $V\gtrsim W$ in the Loewner sense, i.e $V-W$ is positive semidefinite. Define $z:=V^{-1}W(x-\tilde y)$ and $y^+:=\tilde y+\theta z$, with $0\leq\theta\leq 2$. Then $\sigma(y^+)\leq\sigma(\tilde y)$.
:::

::: {.proof}
Because $y=\tilde y+(y-\tilde y)$ we have
\begin{equation}
\sigma(y)=
\sigma(\tilde y)-2(y-\tilde y)'W(x-\tilde y)+(y-\tilde y)'W(y - \tilde y).
\end{equation}
Define 
\begin{equation}
\eta(y,\tilde y):=
\sigma(\tilde y)-2(y-\tilde y)'W(x-\tilde y)+(y-\tilde y)'V(y - \tilde y).
\end{equation}
then $\sigma(y)\leq\eta(y,\tilde y)$ with equality if $y=\tilde y$.
Now $y^+-\tilde y=\theta z$ and thus
\begin{equation}
\sigma(y^+)\leq\eta(y^+,\tilde y)=\sigma(\tilde y)+(\theta^2-2\theta)z'Vz\leq \sigma(\tilde y).
\end{equation}
:::


 majorization
This is the *sandwich inequality* from majorization theory, which says that a
majorization step decreases the loss function value. 

Suppose $y^+\neq\tilde y$. Then the inequality
$\sigma(y^+)\leq\eta(y^+,\tilde y)$ is strict if the minimum of $\eta(y,\tilde y)$
is unique, which will be the case if $V$ is positive definite. The second inequality $\eta(y^+,\tilde y)\leq\eta(\tilde y,\tilde y)$ is strict if
$V-W$ is positive definite. Since $W$ is positive semidefinite both inequalities are strict if $V-W$ is positive definite.

\begin{equation}
y^{(k+1)}\in\mathop{\text{argmin}}_{y\in\mathcal{Y}}\eta(y,y^{(k)})
\end{equation}

Completing the square. Let $z:=V^{-1}W(x-\tilde y)=V^{-1}\nabla\sigma(\tilde y)$
$$
\eta(y,\tilde y)=
\sigma(\tilde y)-2(y-\tilde y)'Vz+(y-\tilde y)'V(y - \tilde y)=
\sigma(\tilde y)+(y-(\tilde y+z))'V(y-(\tilde y+z))-z'Vz
$$
$$
y^{(k+1)}\in\Pi_{\mathcal{Y}}^V(\tilde y+z)
$$
$$
z=\theta V^{-1}W(x-\tilde y)
$$

## Scalar Majorization

$V=\nu D$. $\nu D\gtrsim W$ $\nu I\gtrsim D^{-1}W$ $\nu\geq\lambda_{\text{max}}(D^{-1}W)$

## Diagonal Majorization

Find the smallest diagonal matrix $V\gtrsim W$. We define "smallest" as
minimizing the trace $\text{tr}\ V$.
Minimizing $f$ over $V\gtrsim W$ is a convex problem, which we can
formulate as the linear semi-infinite problem of minimizing $f$
over $V$ that satisfies $x'Vx\geq x'Wx$ for all $x'x=1$.

The R program myCutter() is a simple cutting plane algorithm to do just that.
It is not optimized in any way, and it will probably fail for large
$n$. The basic idea is to approximate the feasible region $V\gtrsim W$
from the outside by a compact convex polyhedron. 
We start with the $n$ inequalities $v_{ii}\geq w_{ii}$. 
Then minimize the objective function by linear programming (using lpSolve, @berkelaar_24). This will give a minimum attained at a vertex of the polyhedron. At this vertex the smallest eigenvalue (computed with RSpectra, @qiu_mei_24) of $V-W$ will be negative. If $x$ is the corresponding normalized eigenvector then we add the contraint $x'Vx\geq x'Wx$ and solve the linear program with this added inequality,
which cuts off the vertex that defined the current solution. The minimum valueof the loss function will increase, and we continue until we have convergence.

Thus the number of inequality constraints increases by one in each iteration.
The algorithm is wasteful because it does not prune the inequalities, and
because each linear programming solution is started from scratch, without
taking into account that two successive programs only differ by one
inequality constraint.

## Examples

### Linear Regression

### Low-rank Approximation