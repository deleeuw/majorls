---
title: Majorizing Least Squares Loss functions
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
```

\sectionbreak 

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/majorls> 

\sectionbreak

# Introduction

## Majorization

# General

The components of our problem are

* $x$, a vector with $n$ elements, 
* $W$, a positive semi-definite matrix of order $n$, 
* $\mathcal{Y}$, some subset of $\mathbb{R}^n$.

Define the least squares loss function
$$
\sigma(y):=\tfrac12(x-y)'W(x-y).
$$
Our problem is to minimize $\sigma$ over $y\in\mathcal{Y}$.

Suppose $\tilde y$ is some element of $\mathcal{Y}$, with loss function value
$\sigma(\tilde y)$. We want to find an element of $\mathcal{Y}$ which is
better than $\tilde y$, i.e. has a lower value of loss.
Because $y=\tilde y+(y-\tilde y)$ we have
\begin{equation}
\sigma(y)=
\sigma(\tilde y)-2(y-\tilde y)'W(x-\tilde y)+(y-\tilde y)'W(y - \tilde y)
\end{equation}
Suppose $V$ is another positive semidefinite matrix with $V\gtrsim W$ in the Loewner sense, i.e $V-W$ is positive semidefinite.
Define 
\begin{equation}
\eta(y,\tilde y):=
\sigma(\tilde y)-2(y-\tilde y)'W(x-\tilde y)+(y-\tilde y)'V(y - \tilde y)
\end{equation}
then $\sigma(y)\leq\eta(y,\tilde y)$ with equality if $y=\tilde y$. 
Now suppose $y^+$ minimizes $\eta(y,\tilde y)$ over $\mathcal{Y}$. Then
\begin{equation}
\sigma(y^+)\leq\eta(y^+,\tilde y)\leq\eta(\tilde y,\tilde y)=\sigma(\tilde y)
\end{equation}
This is the *sandwich inequality* from majorization theory, which says that a
majorization step decreases the loss function value. 

Suppose $y^+\neq\tilde y$. Then the inequality
$\sigma(y^+)\leq\eta(y^+,\tilde y)$ is strict if the minimum of $\eta(y,\tilde y)$
is unique, which will be the case if $V$ is positive definite. The second inequality $\eta(y^+,\tilde y)\leq\eta(\tilde y,\tilde y)$ is strict if
$V-W$ is positive definite. Since $W$ is positive semidefinite both inequalities are strict if $V-W$ is positive definite.

\begin{equation}
y^{(k+1)}\in\mathop{\text{argmin}}_{y\in\mathcal{Y}}\eta(y,y^{(k)})
\end{equation}

Completing the square. Let $z:=V^{-1}W(x-\tilde y)=V^{-1}\nabla\sigma(\tilde y)$
$$
\eta(y,\tilde y)=
\sigma(\tilde y)-2(y-\tilde y)'Vz+(y-\tilde y)'V(y - \tilde y)=
\sigma(\tilde y)+(y-(\tilde y+z))'V(y-(\tilde y+z))-z'Vz
$$
$$
y^{(k+1)}\in\Pi_{\mathcal{Y}}^V(\tilde y+z)
$$
$$
z=\theta V^{-1}W(x-\tilde y)
$$

## Scalar Majorization

$V=\nu D$. $\nu D\gtrsim W$ $\nu I\gtrsim D^{-1}W$ $\nu\geq\lambda_{\text{max}}(D^{-1}W)$

## Diagonal Majorization

Find the smallest diagonal matrix $V\gtrsim W$. We define "smallest" as
minimizing the trace $\text{tr}\ V$.
Minimizing $f$ over $V\gtrsim W$ is a convex problem, which we can
formulate as the linear semi-infinite problem of minimizing $f$
over $V$ that satisfies $x'Vx\geq x'Wx$ for all $x'x=1$.

The R program .. is a simple cutting plane algorithm to do just that.
It is not optimized in any way, and it will probably fail for large 
$n$. We start with the $n$ inequalities $v\geq w$, where $v:=\text{diag(v)}$ and $w:=\text{diag(W)}$. This gives $v^{(1)}=w$ and $f(v^{(1)})=\text{tr}(W)$.
If we have a solution $v^{(k)}$ then we compute the smallest eigenvalue
of $\lambda_{\text{min}}(V^{(k)}-W)$ and the corresponding eigenvector $x^{(k)}$. If
the eigenvalue


## Examples

### Linear Regression

### Low-rank Approximation